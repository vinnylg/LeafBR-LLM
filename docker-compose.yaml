version: '3.8'

services:
  llama_dev_container:
    build:
      context: .
      dockerfile: Dockerfile  # Dockerfile para buildar a imagem personalizada
    container_name: llama_dev_container
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility  # Ativar as capacidades de GPU necessárias
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    volumes:
      - ./:/workspace  # Monta o diretório inteiro no container para desenvolvimento
      - ./huggingface:/root/.cache/huggingface  # Cache local Hugging Face

    ports:
      - "8888:8888"  # Mapeamento de portas (se você rodar um Jupyter ou algo similar)

    runtime: nvidia  # Utiliza o runtime NVIDIA para suportar operações com GPU

    stdin_open: true  # Manter STDIN aberto (útil para terminais interativos)
    tty: true         # Ativar TTY para sessões interativas (útil para desenvolvimento)

    command: bash  # Iniciar o container com um terminal bash para desenvolvimento
